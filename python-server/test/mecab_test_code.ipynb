{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### //FIXME : 추후 사용자 사전을 작성하여 성능을 향상 시킬 필요성이 있음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-mecab-ko\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from krwordrank.word import KRWordRank\n",
    "from mecab import MeCab\n",
    "mecab = MeCab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"서울 지진 피해에 대한 데이터 분석을 위해서는 어떤 종류의 데이터를 사용해야 할지 먼저 생각해보아야 합니다. 예를 들어, 지진 발생 시간, 지진 규모, 지진 발생 지역, 피해 규모 등의 정보가 필요할 것입니다. 서울 지진 피해 분석 예시: 서울 지역에서 최근 몇 년간 발생한 지진 데이터를 수집하여 지진 발생 건수, 지진 규모, 지진 발생 지역 등의 정보를 파악할 수 있습니다. 이를 바탕으로 서울 지역에서 지진 발생이 가장 많은 지역, 지진 규모와 피해 규모 간의 상관 관계, 지진 발생 시간대 등을 분석할 수 있습니다. 또한, 특정 지역에서의 지진 발생 시 피해 규모가 어떻게 나타나는지 분석하여 지진 대비 대응 전략을 마련할 수 있습니다. 서울 지진에 대한 데이터는 국가지진정보센터에서 제공하는 '국내 지진 정보 시스템'에서 확인할 수 있습니다. 이 시스템에서는 지난 1년간의 국내 지진 정보를 확인할 수 있으며, 서울 지역에서 발생한 지진 정보도 포함되어 있습니다. 이를 바탕으로 데이터를 수집하고 분석할 수 있습니다.\"\n",
    "text = \"\"\"\n",
    "폐식용유나 생활 폐기물 등으로 만든 친환경 항공유를 연료로 사용하는 항공기가 국내에서 처음으로 시범 운항에 돌입합니다. 국토교통부와 산업통상자원부는 인천∼LA 노선을 운항하는 대한항공 화물기에 바이오항공유, SAF를 급유해 석 달간 시범 운항을 한다고 밝혔습니다. SAF는 '지속가능 항공유'라는 의미로, 기존 화석연료 기반 항공유보다 탄소 배출을 최대 80%까지 줄일 수 있고 기존 항공유와도 혼합해 사용할 수 있습니다. 오늘 첫 시범 운항에는 오후 5시 45분 인천국제공항에서 이륙하는 대한항공 KE207편이 투입됩니다. 우선 SAF를 2% 섞은 항공유를 급유해 한 달에 2차례씩 석 달간 모두 6차례 시범 운항을 합니다. 시범 운항에 사용하는 SAF는 대한항공과 바이오항공유 실증 추진 협약을 맺은 GS칼텍스가 공급합니다. GS칼텍스는 세계 최대 바이오연료 생산 기업인 핀란드 '네스테'사가 생산한 바이오항공유를 국내 최초로 공급받아 대한항공 화물기에 급유합니다. 이 제품은 미국재료시험협회 등 국제 품질 기준을 충족한 바 있습니다. ※ '당신의 제보가 뉴스가 됩니다' [카카오톡] YTN 검색해 채널 추가 [전화] 02-398-8585 [메일] social@ytn.co.kr\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'폐식용유 생활 폐기물 등 친환경 항공 유 연료 사용 항공기 국내 처음 시범 운항 돌입 국토 교통부 산업 통상 자원 부 인천 노선 운항 대한항공 화물기 바이오 항공 급유 달 간 시범 운항 지속 가능 항공 유 의미 기존 화석 연료 기반 항공 유 탄소 배출 최대 수 기존 항공 유와 혼합 사용 수 시범 운항 오후 시 분 인천 국제 공항 이륙 대한항공 편 투입 항공 유 급유 달 차례 달 간 차례 시범 운항 시범 운항 사용 대한항공 바이오 항공 유 실증 추진 협약 칼 텍스 공급 칼 텍스 세계 최대 바이오 연료 생산 기업 핀란드 네스 테 사 생산 바이오 항공 유 국내 최초 공급 항공 화물기 급유 제품 미국 재료 시험 협회 등 국제 품질 기준 충족 바 당신 제보 뉴스 카카오톡 검색 채널 추가 전화 메일'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from mecab import MeCab\n",
    "# mecab = MeCab()\n",
    "result_text = ' '.join(mecab.nouns(text))\n",
    "\n",
    "result_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      항공:\t2.6219\n",
      "     바이오:\t1.8499\n",
      "      운항:\t1.5989\n",
      "     화물기:\t1.1540\n",
      "      사용:\t1.1518\n"
     ]
    }
   ],
   "source": [
    "# from krwordrank.word import KRWordRank\n",
    "min_count = 2   # 단어의 최소 출현 빈도수 (그래프 생성 시)\n",
    "max_length = 10 # 단어의 최대 길이\n",
    "wordrank_extractor = KRWordRank(min_count=min_count, max_length=max_length)\n",
    "beta = 0.90    # PageRank의 decaying factor beta\n",
    "max_iter = 20\n",
    "texts = [result_text]\n",
    "keywords, rank, graph = wordrank_extractor.extract(texts, beta, max_iter)\n",
    "for word, r in sorted(keywords.items(), key=lambda x:x[1], reverse=True)[:5]:\n",
    "        print('%8s:\\t%.4f' % (word, r))\n",
    "# 2번 등장을 기준으로 하면 조금 달라진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from krwordrank.word import KRWordRank\n",
    "from mecab import MeCab\n",
    "\n",
    "class KeywordService:\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        self.mecab = MeCab()\n",
    "        \n",
    "        min_count = 2   # 단어의 최소 출현 빈도수 (그래프 생성 시)\n",
    "        max_length = 10 # 단어의 최대 길이\n",
    "        self.wordrank_extractor = KRWordRank(min_count=min_count, max_length=max_length)\n",
    "        \n",
    "        self.beta = 0.90    # PageRank의 decaying factor beta\n",
    "        self.max_iter = 20\n",
    "        self.extract_length = 5 # 우선순위로 정렬 후, 추출할 키워드의 갯수 \n",
    "    \n",
    "    # text를 받아 키워드를 추출하는 함수\n",
    "    # str -> dict\n",
    "    def get_keyword(self, text:str):\n",
    "        res = dict()\n",
    "        \n",
    "        result_text = ' '.join(self.mecab.nouns(text))\n",
    "        keywords, rank, graph = self.wordrank_extractor.extract([result_text], self.beta, self.max_iter)\n",
    "        for word, r in sorted(keywords.items(), key=lambda x:x[1], reverse=True)[:self.extract_length]:\n",
    "            #TODO: 가중치는 보완 필요\n",
    "            res[word] = r\n",
    "            \n",
    "        return res\n",
    "                \n",
    "    # 여러개의 text를 list로 받아 키워드를 추출하는 함수\n",
    "    # list -> dict\n",
    "    def get_keywords(self, texts:list):\n",
    "        res = dict()\n",
    "        \n",
    "        # 각 텍스트의 키워드들을 합쳐준다.\n",
    "        for text in texts:\n",
    "            keywords = self.get_keyword(text)\n",
    "            for key, value in keywords.items():\n",
    "                if key in res:\n",
    "                    res[key] += value\n",
    "                else:\n",
    "                    res[key] = value\n",
    "        \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'항공': 2.6218525638983485,\n",
       " '바이오': 1.8499463510853484,\n",
       " '운항': 1.5989188649166284,\n",
       " '화물기': 1.1539625024028535,\n",
       " '사용': 1.1517840591962294}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service = KeywordService()\n",
    "service.get_keyword(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Obtaining dependency information for openai from https://files.pythonhosted.org/packages/ae/59/911d6e5f1d7514d79c527067643376cddcf4cb8d1728e599b3b03ab51c69/openai-0.28.0-py3-none-any.whl.metadata\n",
      "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\ssafy\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ssafy\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (4.66.1)\n",
      "Collecting aiohttp (from openai)\n",
      "  Obtaining dependency information for aiohttp from https://files.pythonhosted.org/packages/21/4e/452858698e53ddf06ea137eac268db535c9605394c27236f9986168dd82f/aiohttp-3.8.5-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading aiohttp-3.8.5-cp311-cp311-win_amd64.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ssafy\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.20->openai) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ssafy\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ssafy\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.20->openai) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ssafy\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.20->openai) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ssafy\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->openai) (23.1.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->openai)\n",
      "  Downloading multidict-6.0.4-cp311-cp311-win_amd64.whl (28 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->openai)\n",
      "  Obtaining dependency information for async-timeout<5.0,>=4.0.0a3 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->openai)\n",
      "  Downloading yarl-1.9.2-cp311-cp311-win_amd64.whl (60 kB)\n",
      "     ---------------------------------------- 0.0/60.2 kB ? eta -:--:--\n",
      "     -------------------- ------------------- 30.7/60.2 kB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 60.2/60.2 kB 1.6 MB/s eta 0:00:00\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->openai)\n",
      "  Obtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/39/16/72d9ccd30815d0b37218348f053be37bc3d14288ac192a794a39990ac28e/frozenlist-1.4.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading frozenlist-1.4.0-cp311-cp311-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->openai)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\ssafy\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm->openai) (0.4.6)\n",
      "Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
      "   ---------------------------------------- 0.0/76.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 76.5/76.5 kB 4.4 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.8.5-cp311-cp311-win_amd64.whl (320 kB)\n",
      "   ---------------------------------------- 0.0/320.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 320.6/320.6 kB 10.0 MB/s eta 0:00:00\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.0-cp311-cp311-win_amd64.whl (44 kB)\n",
      "   ---------------------------------------- 0.0/44.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 44.9/44.9 kB 2.2 MB/s eta 0:00:00\n",
      "Installing collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
      "Successfully installed aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.3 frozenlist-1.4.0 multidict-6.0.4 openai-0.28.0 yarl-1.9.2\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이거임'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input(\"입력은:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list = [1,2,3,4,5,6,7]\n",
    "test_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'service' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m json\u001b[39m.\u001b[39mdumps(service\u001b[39m.\u001b[39mget_keyword(test_list), ensure_ascii\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'service' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "json.dumps(service.get_keyword(test_list), ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = dict()\n",
    "a[1] = 11\n",
    "a[2] = 22\n",
    "a[3] = 33\n",
    "\n",
    "list(a.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
